{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea50def",
   "metadata": {},
   "source": [
    "### Import the packages that are needed for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe6ba72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Obtaining dependency information for wordcloud from https://files.pythonhosted.org/packages/34/ac/72a4e42e76bf549dfd91791a6b10a9832f046c1d48b5e778be9ec012aa47/wordcloud-1.9.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading wordcloud-1.9.2-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wordcloud) (1.24.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wordcloud) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.2-cp311-cp311-win_amd64.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/151.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/151.4 kB ? eta -:--:--\n",
      "   ------- ------------------------------- 30.7/151.4 kB 262.6 kB/s eta 0:00:01\n",
      "   ---------- ---------------------------- 41.0/151.4 kB 279.3 kB/s eta 0:00:01\n",
      "   ----------------------- --------------- 92.2/151.4 kB 476.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- 151.4/151.4 kB 644.7 kB/s eta 0:00:00\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.2\n"
     ]
    }
   ],
   "source": [
    "! pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341585b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import STOPWORDS,WordCloud\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel,CoherenceModel\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "#import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "#import pyLDAvis\n",
    "from random import shuffle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np \n",
    "from joblib import dump, load \n",
    "from scipy.sparse import save_npz, load_npz \n",
    "from scipy.stats import uniform\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0e7da9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Successfully Saved.!!!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "class ExtractTweets:\n",
    "    \"\"\"\n",
    "    - class for extracting tweets\n",
    "    \"\"\"\n",
    "    def __init__(self,json_file: str)->list:\n",
    "        data = []\n",
    "        for tweets in open(json_file,'r'):\n",
    "            data.append(json.loads(tweets))\n",
    "        self.tweets = data\n",
    "\n",
    "    def get_tweets(self):\n",
    "        \"\"\"just a basic function of getting tweets\"\"\"\n",
    "        return self.tweets\n",
    "\n",
    "    def find_statuses_count(self)->list:\n",
    "        \"\"\"\n",
    "        - this function returns the status counts\n",
    "        \"\"\"\n",
    "        return list(map(lambda tweet: tweet['user']['statuses_count'], self.tweets))\n",
    "    \n",
    "    def find_full_text(self)->list:\n",
    "        \"\"\"\n",
    "        - this function extracts the full texts of\n",
    "        the tweets\n",
    "        \"\"\"\n",
    "        return list(map(lambda tweet: tweet['text'], self.tweets))\n",
    "        \n",
    "\n",
    "    def find_sentiments(self, data:list)->list:\n",
    "        \"\"\"\n",
    "        - this function finds sentiments from the dataset\n",
    "        \"\"\"\n",
    "        get_polarity = [TextBlob(text).sentiment.polarity for text in data]\n",
    "        get_subjectivity = [TextBlob(text).sentiment.subjectivity for text in data]\n",
    "        return get_polarity, get_subjectivity\n",
    "\n",
    "    def find_created_time(self)->list:\n",
    "        \"\"\"\n",
    "        - this function returns a list of \n",
    "        the created time tags for when the tweet was generated\n",
    "        \"\"\"\n",
    "        return  list(map(lambda tweet: tweet['created_at'], self.tweets))\n",
    "\n",
    "\n",
    "    def find_source(self)->list:\n",
    "        \"\"\"\n",
    "        - this function returns the source of the tweet\n",
    "        \"\"\"\n",
    "        return  list(map(lambda tweet: tweet['source'], self.tweets))\n",
    "        \n",
    "    def find_screen_name(self)->list:\n",
    "        \"\"\"\n",
    "        - this function returns the screen name of the person who has tweeted\n",
    "        \"\"\"\n",
    "        return  list(map(lambda tweet: tweet['user']['screen_name'], self.tweets))\n",
    "        \n",
    "    def find_followers_count(self)->list:\n",
    "        \"\"\"\n",
    "        - this function returns the amount of followers per user\n",
    "        \"\"\"\n",
    "        return  list(map(lambda tweet: tweet['user']['followers_count'], self.tweets))\n",
    "        \n",
    "    def find_friends_count(self)->list:\n",
    "        \"\"\"\n",
    "        - this function returns the number of friends the user has\n",
    "        \"\"\"\n",
    "        return  list(map(lambda tweet: tweet['user']['friends_count'], self.tweets))\n",
    "        \n",
    "    def is_sensitive(self)->list:\n",
    "        \"\"\"\n",
    "        - this function checks whether the data is sensitive or not\n",
    "        \"\"\"\n",
    "        return [tweet['possibly_sensitive'] if \"possibly_sensitive\" in tweet.keys() else None \\\n",
    "        for tweet in self.tweets]\n",
    "               \n",
    "\n",
    "    def find_favourite_count(self)->list:\n",
    "        \"\"\"\n",
    "        - this function returns the amount of times the tweet has been counted\n",
    "        as favorite\n",
    "        \"\"\"\n",
    "        return [tweet['retweeted_status']['favorite_count'] if 'retweeted_status' in tweet.keys() else 0 \\\n",
    "        for tweet in self.tweets]\n",
    "        \n",
    "    def find_retweet_count(self)->list:\n",
    "        \"\"\"\n",
    "        - this function finds how many times a tweet has been retweeted\n",
    "        \"\"\"\n",
    "        return [tweet['retweeted_status']['retweet_count'] if 'retweeted_status' in tweet.keys() else 0 \\\n",
    "        for tweet in self.tweets]\n",
    "        \n",
    "    \n",
    "    def find_hashtags(self) -> list:\n",
    "        \"\"\"\n",
    "        return the amount of hashtags in tweets\n",
    "        \"\"\"\n",
    "        return [tweet.get('entities',dict()).get('hashtags', None)\n",
    "                    for tweet in self.tweets]\n",
    "\n",
    "        \n",
    "    def find_mentions(self)->list:\n",
    "        \"\"\"\n",
    "        - this function returns how many times \n",
    "        a person was mentioned in a tweet\n",
    "        \"\"\"\n",
    "        return [\" , \".join([count_['screen_name']  for tweet in self.tweets for count_ in tweet['entities']['user_mentions']])]\n",
    "    \n",
    "    def find_lang(self)->list:\n",
    "        \"\"\"\n",
    "        return the language used to tweet\n",
    "        \"\"\"\n",
    "        return list(map(lambda tweet:tweet['lang'],self.tweets))\n",
    "\n",
    "    def find_location(self)->list:\n",
    "        \"\"\"\n",
    "        returns the location in which the tweet was published\n",
    "        \"\"\"\n",
    "        return [tweet['user']['location'] for tweet in self.tweets]\n",
    "        \n",
    "    def get_tweet_df(self, save=False)->pd.DataFrame:\n",
    "        \"\"\"required column to be generated you should be creative and add more features\"\"\"\n",
    "        \n",
    "        columns = ['created_at', 'source', 'original_text','polarity','subjectivity', 'lang', 'favorite_count', 'retweet_count', \n",
    "            'original_author', 'followers_count','friends_count','possibly_sensitive', 'hashtags', 'user_mentions', 'place']\n",
    "        \n",
    "        created_at = self.find_created_time()\n",
    "        source = self.find_source()\n",
    "        text = self.find_full_text()\n",
    "        polarity, subjectivity = self.find_sentiments(text)\n",
    "        lang = self.find_lang()\n",
    "        fav_count = self.find_favourite_count()\n",
    "        retweet_count = self.find_retweet_count()\n",
    "        screen_name = self.find_screen_name()\n",
    "        follower_count = self.find_followers_count()\n",
    "        friends_count = self.find_friends_count()\n",
    "        sensitivity = self.is_sensitive()\n",
    "        hashtags = self.find_hashtags()\n",
    "        mentions = self.find_mentions()\n",
    "        location = self.find_location()\n",
    "        values = [created_at, source, text, polarity, subjectivity, lang, fav_count, retweet_count, screen_name, follower_count, friends_count, sensitivity, hashtags, mentions, location]\n",
    "        data_ = dict(zip(columns,values))\n",
    "        data  = { key:pd.Series(value) for key, value in data_.items() }\n",
    "        df = pd.DataFrame(data=data)\n",
    "        \n",
    "        if save:\n",
    "            df.to_csv('C:/Users/Admin/Desktop/Desmondonam/Data_Science/Week0/Twitter-Data-Analysis/data/processed_tweet_data.csv', index=False)\n",
    "            print('File Successfully Saved.!!!')\n",
    "            \n",
    "        return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    extracted_tweets = ExtractTweets(r\"C:\\Users\\Admin\\Desktop\\Desmondonam\\Data_Science\\Week0\\Twitter-Data-Analysis\\data\\Economic_Twitter_Data_minified.json\")\n",
    "    df = extracted_tweets.get_tweet_df(save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0394df62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automation in Action...!!!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# import enchant\n",
    "\n",
    "\n",
    "\n",
    "class TweetCleanser:\n",
    "    \"\"\"\n",
    "    -this class cleans the tweets and\n",
    "    ensures that the data is easy to work with\n",
    "    \"\"\"\n",
    "    # en_us = enchant.Dict(\"en_US\")\n",
    "\n",
    "    def __init__(self, df:pd.DataFrame):\n",
    "        self.df = df\n",
    "        print('Automation in Action...!!!')\n",
    "\n",
    "    def drop_unwanted_column(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        remove rows that has column names. This error originated from\n",
    "        the data collection stage.  \n",
    "        \"\"\"\n",
    "        unwanted_rows = self.df[self.df['retweet_count'] == 'retweet_count' ].index\n",
    "        self.df.drop(unwanted_rows , inplace=True)\n",
    "        self.df = self.df[self.df['polarity'] != 'polarity']\n",
    "        return self.df\n",
    "        \n",
    "    \n",
    "    def drop_duplicate(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        - this function drop duplicate rows\n",
    "        \"\"\"\n",
    "        self.df = self.df.drop_duplicates(subset=['original_text'])\n",
    "        return self.df\n",
    "        \n",
    "    def convert_to_datetime(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        convert column to datetime\n",
    "        \"\"\"\n",
    "        self.df['created_at'] = pd.to_datetime(self.df['created_at'])\n",
    "        return self.df\n",
    "    \n",
    "    def convert_to_numbers(self, df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        convert columns like polarity, subjectivity, retweet_count\n",
    "        favorite_count etc to numbers\n",
    "        \"\"\"\n",
    "        for key in self.df.columns:\n",
    "            if self.df[key].dtype == 'float64':\n",
    "                self.df[key] = self.df[key].astype(int)\n",
    "        return self.df\n",
    "    \n",
    "    def remove_non_english_tweets(self,df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        remove non english tweets from lang that is the language \n",
    "        column\n",
    "        \"\"\"\n",
    "        self.df = self.df[self.df['lang'].str.contains(\"en\")]\n",
    "        return self.df\n",
    "\n",
    "    def get_hashtags(self,tweet):\n",
    "        '''This function will extract hashtags'''\n",
    "        return re.findall('(#[A-Za-z]+[A-Za-z0-9-_]+)', tweet)\n",
    "\n",
    "    def save_changes(self)->pd.DataFrame:\n",
    "        self.df.to_csv(r\"C:\\Users\\Admin\\Desktop\\Desmondonam\\Data_Science\\Week0\\Twitter-Data-Analysis\\data\\cleaned_data.csv\", index=False)\n",
    "\n",
    "    # def clean_text(self,tweet):\n",
    "    #     \"\"\"this function cleans the original text\"\"\"\n",
    "    #     return ' '.join(w for w in tweet.split() if self.en_us.check(w))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(r\"C:\\Users\\Admin\\Desktop\\Desmondonam\\Data_Science\\Week0\\Twitter-Data-Analysis\\data\\processed_tweet_data.csv\")\n",
    "    cleanser = TweetCleanser(df)\n",
    "    cleanser.drop_unwanted_column(df)\n",
    "    cleanser.drop_duplicate(df)\n",
    "    cleanser.convert_to_datetime(df)\n",
    "    cleanser.remove_non_english_tweets(df)\n",
    "    cleanser.save_changes()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d221c17",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6187006d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>source</th>\n",
       "      <th>original_text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>lang</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [created_at, source, original_text, polarity, subjectivity, lang, favorite_count, retweet_count, original_author, followers_count, friends_count, possibly_sensitive, hashtags, user_mentions, place]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe from extracted tweets\n",
    "extracted_tweets = ExtractTweets(r\"C:\\Users\\Admin\\Desktop\\Desmondonam\\Data_Science\\Week0\\Twitter-Data-Analysis\\data\\Economic_Twitter_Data_minified.json\")\n",
    "df = extracted_tweets.get_tweet_df(save=False)\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662d021",
   "metadata": {},
   "source": [
    "Processing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c50a4f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automation in Action...!!!\n"
     ]
    }
   ],
   "source": [
    "# clean the dataframe\n",
    "cleanser = TweetCleanser(df)\n",
    "# drop unwanted columns\n",
    "cleanser.drop_unwanted_column(df)\n",
    "# drop duplicate values from original text\n",
    "cleanser.drop_duplicate(df)\n",
    "# convert date data to appropriate datetime\n",
    "cleanser.convert_to_datetime(df)\n",
    "# remove non english texts\n",
    "df_ = cleanser.remove_non_english_tweets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0f80488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>source</th>\n",
       "      <th>original_text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>lang</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2022-04-22 22:17:05+00:00</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>RT @NorthstarCharts: The 10-year yield is tell...</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>en</td>\n",
       "      <td>188</td>\n",
       "      <td>43</td>\n",
       "      <td>davideiacovozzi</td>\n",
       "      <td>18</td>\n",
       "      <td>55</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'text': 'gold', 'indices': [116, 121]}, {'te...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2022-04-22 13:44:53+00:00</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>RT @MichaelAArouet: German 10y mortgage rate w...</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>en</td>\n",
       "      <td>179</td>\n",
       "      <td>32</td>\n",
       "      <td>davideiacovozzi</td>\n",
       "      <td>18</td>\n",
       "      <td>55</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2022-04-22 06:10:34+00:00</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>RT @goldseek: When? https://t.co/kO2FfHKaZg</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>193</td>\n",
       "      <td>26</td>\n",
       "      <td>davideiacovozzi</td>\n",
       "      <td>18</td>\n",
       "      <td>55</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2022-04-21 17:22:09+00:00</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>RT @charliebilello: The 30-year mortgage rate ...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>en</td>\n",
       "      <td>620</td>\n",
       "      <td>213</td>\n",
       "      <td>davideiacovozzi</td>\n",
       "      <td>18</td>\n",
       "      <td>55</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2022-04-21 10:32:26+00:00</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>RT @biancoresearch: Rates rise until something...</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>en</td>\n",
       "      <td>1787</td>\n",
       "      <td>417</td>\n",
       "      <td>davideiacovozzi</td>\n",
       "      <td>18</td>\n",
       "      <td>55</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at  \\\n",
       "38 2022-04-22 22:17:05+00:00   \n",
       "39 2022-04-22 13:44:53+00:00   \n",
       "41 2022-04-22 06:10:34+00:00   \n",
       "42 2022-04-21 17:22:09+00:00   \n",
       "43 2022-04-21 10:32:26+00:00   \n",
       "\n",
       "                                               source  \\\n",
       "38  <a href=\"http://twitter.com/download/android\" ...   \n",
       "39  <a href=\"http://twitter.com/download/android\" ...   \n",
       "41  <a href=\"http://twitter.com/download/android\" ...   \n",
       "42  <a href=\"http://twitter.com/download/android\" ...   \n",
       "43  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "                                        original_text  polarity  subjectivity  \\\n",
       "38  RT @NorthstarCharts: The 10-year yield is tell...      0.16      0.540000   \n",
       "39  RT @MichaelAArouet: German 10y mortgage rate w...      0.15      0.175000   \n",
       "41        RT @goldseek: When? https://t.co/kO2FfHKaZg      0.00      0.000000   \n",
       "42  RT @charliebilello: The 30-year mortgage rate ...      0.00      0.183333   \n",
       "43  RT @biancoresearch: Rates rise until something...     -0.40      0.400000   \n",
       "\n",
       "   lang  favorite_count  retweet_count  original_author  followers_count  \\\n",
       "38   en             188             43  davideiacovozzi               18   \n",
       "39   en             179             32  davideiacovozzi               18   \n",
       "41   en             193             26  davideiacovozzi               18   \n",
       "42   en             620            213  davideiacovozzi               18   \n",
       "43   en            1787            417  davideiacovozzi               18   \n",
       "\n",
       "    friends_count possibly_sensitive  \\\n",
       "38             55               None   \n",
       "39             55               None   \n",
       "41             55              False   \n",
       "42             55               None   \n",
       "43             55              False   \n",
       "\n",
       "                                             hashtags user_mentions place  \n",
       "38  [{'text': 'gold', 'indices': [116, 121]}, {'te...           NaN        \n",
       "39                                                 []           NaN        \n",
       "41                                                 []           NaN        \n",
       "42                                                 []           NaN        \n",
       "43                                                 []           NaN        "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aca78ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 16 entries, 38 to 54\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count  Dtype              \n",
      "---  ------              --------------  -----              \n",
      " 0   created_at          16 non-null     datetime64[ns, UTC]\n",
      " 1   source              16 non-null     object             \n",
      " 2   original_text       16 non-null     object             \n",
      " 3   polarity            16 non-null     float64            \n",
      " 4   subjectivity        16 non-null     float64            \n",
      " 5   lang                16 non-null     object             \n",
      " 6   favorite_count      16 non-null     int64              \n",
      " 7   retweet_count       16 non-null     int64              \n",
      " 8   original_author     16 non-null     object             \n",
      " 9   followers_count     16 non-null     int64              \n",
      " 10  friends_count       16 non-null     int64              \n",
      " 11  possibly_sensitive  5 non-null      object             \n",
      " 12  hashtags            16 non-null     object             \n",
      " 13  user_mentions       0 non-null      object             \n",
      " 14  place               16 non-null     object             \n",
      "dtypes: datetime64[ns, UTC](1), float64(2), int64(4), object(8)\n",
      "memory usage: 2.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f0be0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for any missing values from the data\n",
    "missing_values = df_.isnull().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfbf1996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "361aabe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the columns that have values\n",
    "columns_with_null_values = df_.columns[df_.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d1625cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['possibly_sensitive', 'user_mentions'], dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_null_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8194a4",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc10bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate analysis on hashtags\n",
    "def get_hashtags(tweet):\n",
    "    '''This function will extract hashtags'''\n",
    "    return re.findall('(#[A-Za-z]+[A-Za-z0-9-_]+)', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094c41fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f5dc2971426daae6f7c536386a1e4aa85b9246abb2d9b237d641d10e1b238274"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
